import json
import tensorflow as tf
import numpy as np
from prepare_data import prepare_image_data, prepare_text_data

# Common paths.
# TODO(laser): Convert these to arguments for the script.
image_features_path = 'image_features.npy'
text_data_path = 'text_data.json'

# Config
# TODO(laser): Temporary small values here.
batch_size = 1024
# max_images_to_process = 65536
max_images_to_process = 16384
num_epochs = 100
img_feature_count = 2048
word_feature_count = 128

# OPTIMIZE(laser): Data prep step should have everything written to disk in a
# format that tf.data.Dataset can stream. Investigate how Tensorflow does that
# as loading the whole thing into memory takes around 1-1.5 GB of RAM. Also,
# caption processing done here is naive and super slow.
def load_data(image_features_path, text_data_path):
    '''Loads the COCO dataset. Expects a path to one of its json files and to
    a numpy array generated by prepare_image_data.py.'''

    prepare_image_data(output_path=image_features_path)
    prepare_text_data(output_path=text_data_path)

    with open(text_data_path) as f:
        text_data = json.load(f)

    vocabulary = text_data['vocabulary']
    sentence_ids = frozenset(text_data['ids'][:max_images_to_process])
    sentences = text_data['sentences'][:max_images_to_process]
    word_count = len(vocabulary)
    image_count = len(sentences)

    # Load image feature vectors. The loaded numpy array should have image_id as
    # the first column followed by 2048 columns with feature data. ids are
    # sorted to match captions to images easier later on.
    image_features = np.load(image_features_path)
    image_ids = image_features[:, 0]
    image_features = image_features[:, 1 : 2049]

    # Dataset sanity check. Log if any images are lacking captions. Load only
    # valid data into the dataset and ignore any incomplete pairs.
    idx_to_remove = []
    for idx in range(len(image_ids)):
        if not image_ids[idx] in sentence_ids:
            idx_to_remove.append(idx)

    if len(idx_to_remove) > 0:
        print('WARNING: %d images were found without captions, ignoring them.'
              % (len(idx_to_remove)))
        image_ids = np.delete(image_ids, idx_to_remove, axis=0)
        image_features = np.delete(image_features, idx_to_remove, axis=0)

    # TODO(laser): -1 values should be ignored later, and a SparseTensor should
    # be built here insttead.
    pad = len(max(sentences, key=len))
    sentence_array = np.array([i + [-1]*(pad-len(i)) for i in sentences])
    dataset = tf.data.Dataset.from_tensor_slices(
        {'image_id': image_ids,
         'img_features': image_features,
         'sentences': sentence_array})

    print("Data loaded and filtered.")

    return dataset, vocabulary

# Build Model
dataset, vocabulary = load_data(image_features_path, text_data_path)
dataset = dataset.shuffle(100000).batch(batch_size).repeat(num_epochs)
iter = dataset.make_one_shot_iterator()
item = iter.get_next()

# TODO(laser): sentences should be a SparseTensor, figure out what the syntax
# there is and how to get it working with Datasets, which allegedly don't handle
# SparseTensors...
ids, img_features, sentence_words = item['image_id'], item['img_features'], \
                                    item['sentences']

half_batch = int(batch_size/2)
word_count = len(vocabulary)

# TODO(laser): Make the variables trainable (needed backward propagation? other
# mechanism)
W_img = tf.Variable(tf.random_uniform([img_feature_count, word_feature_count]))
word_embeddings = tf.Variable(tf.random_uniform([word_count,
                                                 word_feature_count]))

# The paper shuffles the image-text pairs half way through the model. There's
# really no need to do that so late, and we can keep all the computation linear
# if we shuffle at the start.
labels = tf.constant([1.0] * half_batch + [-1.0] * half_batch)
img_a, img_b = tf.split(img_features, num_or_size_splits=2, axis=0)
img_shuffled = tf.random_shuffle(img_b)
img_merged   = tf.concat([img_a, img_b], 0)

img_embedding = tf.matmul(img_merged, W_img)
E_bow_gather = tf.gather(word_embeddings, sentence_words)
E_bow_sum = tf.reduce_mean(E_bow_gather, 1)

def cosine_similarity(a, b):
    norm_a = tf.nn.l2_normalize(a, 0)
    norm_b = tf.nn.l2_normalize(b, 0)
    return tf.reduce_sum(tf.multiply(norm_a, norm_b), 1)

def pearson(a, b):
    mean_a, var_a = tf.nn.moments(a, 0)
    mean_b, var_b = tf.nn.moments(b, 0)
    cov = 1.0 / (batch_size-1) * tf.reduce_sum((a - mean_a) * (b - mean_b))
    return cov / (var_a * var_b)


similarity = cosine_similarity(img_embedding, E_bow_sum)
p = pearson(similarity, labels)
optimizer = tf.train.AdamOptimizer().minimize(-p)

# Run session
with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    s.run(tf.local_variables_initializer())

    # for epoch in range(num_epochs):
    while True:
        try:
            _, loss = s.run([optimizer, p]);
            print(loss)

        except tf.errors.OutOfRangeError:
            break



